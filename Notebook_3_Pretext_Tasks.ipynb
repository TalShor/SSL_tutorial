{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07715655",
   "metadata": {},
   "source": [
    "# Notebook 3: Pretext Tasks (Rotation, Jigsaw, Colorization)\n",
    "\n",
    "Before the rise of contrastive learning, much of the progress in visual self-supervised learning came from designing pretext tasks – artificial problems that a neural network could solve using the structure of the data itself. These tasks encourage the model to learn high-level features as a by-product of trying to solve them. In this notebook, we will explore three canonical pretext tasks in computer vision:\n",
    "- **Rotation Prediction** – Have the model predict how an image has been rotated.\n",
    "- **Jigsaw Puzzle Solving** – Have the model arrange shuffled image patches into the correct order.\n",
    "- **Colorization** – Have the model colorize a grayscale image.\n",
    "\n",
    "These tasks were important stepping stones in SSL research. While they may not match the representation quality of contrastive methods on their own, they introduced key ideas and are still quite useful or combined with other methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d69e96",
   "metadata": {},
   "source": [
    "## Predicting Image Rotations\n",
    "\n",
    "Gidaris et al. (ICLR 2018) introduced a simple yet effective pretext task: randomly rotate an image by one of four angles {0°, 90°, 180°, 270°}, and train a CNN to classify which rotation was applied. Why does this help? To succeed, the model must understand the content of the image to some degree – for example, a dog upside down vs right-side-up will look very different. The network can’t just detect “sky at top” because not all images have skies, etc.; it has to learn more general features (like object shapes, orientation of canonical structures such as faces) to figure out the rotation.\n",
    "\n",
    "**Task setup:** We define 4 classes corresponding to the 4 possible rotations. For each training image, we randomly rotate it by one of these angles and label it with that angle class (0,1,2,3 for 0°...270°). The network is a standard classifier (like a ResNet) with 4 outputs. It’s trained with a cross-entropy loss to predict the correct rotation.\n",
    "\n",
    "Despite its simplicity, rotation prediction proved to be a powerful self-supervised signal:\n",
    "- It forces the network to learn orientation-dependent features. For instance, many objects have a \"correct\" orientation (animals stand upright, cars on wheels, text is horizontal, etc.). To classify rotation, the network implicitly needs to recognize those objects or at least their asymmetry.\n",
    "- The learned features transfer well to classification and detection tasks. In the original paper, a network pre-trained on rotation prediction significantly outperformed a random-initialized one when fine-tuned on ImageNet or Pascal VOC detection.\n",
    "\n",
    "Training a rotation prediction model is straightforward. Let's implement a quick demonstration on CIFAR-10 (since it has objects where orientation can be meaningful, albeit low-resolution):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0cbf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "# Simple CNN classifier (like earlier SmallCNN but ending in 4 logits for rotation)\n",
    "class RotationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.feat_extractor = SmallCNN(out_dim=64)  # reuse the SmallCNN conv trunk but smaller output\n",
    "        self.classifier = nn.Linear(64, 4)  # 4 rotation classes\n",
    "    def forward(self, x):\n",
    "        feat = self.feat_extractor(x)\n",
    "        logits = self.classifier(feat)\n",
    "        return logits\n",
    "\n",
    "# Create rotated versions of images with corresponding labels\n",
    "def make_rotations(batch_images):\n",
    "    images = []\n",
    "    labels = []\n",
    "    for img in batch_images:\n",
    "        angle = torch.randint(0, 4, (1,)).item() * 90  # random choice of 0,90,180,270\n",
    "        if angle == 0:\n",
    "            rot_img = img\n",
    "            label = 0\n",
    "        else:\n",
    "            rot_img = TF.rotate(img, angle)\n",
    "            label = angle // 90  # 90->1, 180->2, 270->3\n",
    "        images.append(rot_img)\n",
    "        labels.append(label)\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "# Training loop for rotation prediction\n",
    "rot_model = RotationNet().to(device)\n",
    "optimizer = torch.optim.Adam(rot_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We'll use CIFAR-10 data as unlabeled\n",
    "train_loader = DataLoader(train_subset, batch_size=128, shuffle=True)\n",
    "for epoch in range(3):\n",
    "    rot_model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, _ in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        # generate random rotations\n",
    "        imgs_rot, labels_rot = make_rotations(imgs)\n",
    "        imgs_rot, labels_rot = imgs_rot.to(device), labels_rot.to(device)\n",
    "        # forward pass\n",
    "        logits = rot_model(imgs_rot)\n",
    "        loss = criterion(logits, labels_rot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Rotation prediction loss = {total_loss/len(train_loader):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7433e7f0",
   "metadata": {},
   "source": [
    "After training for a few epochs (again, this is just illustrative; real training might use more epochs and a deeper network), the model should be able to predict rotations significantly better than random (which would be 25% accuracy with 4 classes). The learned convolutional filters in `rot_model.feat_extractor` will likely detect edges, corners, and other features that help orient the image.\n",
    "\n",
    "**Feature learning:** What did the model learn? Possibly, it learned to detect gravity direction (sky vs ground), or for objects like animals, the typical orientation of legs/head. For text in images, detecting readable orientation vs upside-down would be key. All these cues are useful for general vision understanding. Indeed, the rotation task was found to complement other tasks: later works combined rotation with contrastive losses to further boost performance, indicating it learns something slightly different and complementary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b169d8db",
   "metadata": {},
   "source": [
    "## Solving Jigsaw Puzzles\n",
    "\n",
    "Another iconic SSL task was proposed by Noroozi & Favaro (ECCV 2016): Jigsaw puzzles. The idea is to take an image, split it into a grid of patches (e.g., 3x3 grid = 9 patches), shuffle these patches, and ask the network to predict the correct order (i.e., which patch goes to which position).  Solving jigsaw puzzles as self-supervision. (a) Shows the original image with grid; (b) the jumbled puzzle given to the network; (c) the solved puzzle in correct arrangement. The model must learn to recognize objects or scenes to some extent to reassemble them correctly.\n",
    "\n",
    "**Formulation:** How do we get a label for a shuffled puzzle? There are many possible permutations (9! = 362,880 for 9 pieces), which is too many classes to classify directly. The authors simplified the problem: they did not allow all permutations, but sampled a subset of 1000 (pre-defined) permutations out of all possibilities and treated the task as 1000-class classification (the network predicts which permutation index is applied). Each permutation is a specific way the tiles could be shuffled, and the model must recognize which one by essentially learning to arrange them.\n",
    "\n",
    "An alternative formulation could be multiple classification heads for each tile’s position, but the original method used the single-class approach.\n",
    "\n",
    "**Network:** Interestingly, they designed a special network (called Context-Free Network) that processes each patch independently (siamese CNN towers with shared weights for each patch) and then a fully connected layer that combines the patch features to predict the permutation. By doing this, they forced the network to truly rely on the spatial configuration rather than trivial cues like continuity between patches (since each patch was processed separately until the final layers).\n",
    "\n",
    "**Learning outcome:** The Jigsaw task is very challenging – the network has to learn the global context of the image. For example, if one patch has a bit of a dog’s face and another has a tail, the network should learn that the face patch likely belongs above the tail patch in the original image. It encourages learning about object parts and their configurations.\n",
    "\n",
    "In practice:\n",
    "- Features learned from jigsaw puzzles were useful for classification and detection. The paper showed improvements on PASCAL VOC detection after using jigsaw pretraining.\n",
    "- The network likely learns to recognize salient objects and textures because aligning patches requires knowing that “this patch has part of a wheel, it should be bottom of a car” etc.\n",
    "\n",
    "We can attempt a simplified jigsaw implementation, but doing a full 1000-way classification is complex for a quick demo. Instead, let's conceptually illustrate a simpler version:\n",
    "\n",
    "We will do a 2x2 puzzle (4 pieces) for demonstration. That yields only 24 possible permutations (4! = 24), which is manageable to brute-force classify in a demo.\n",
    "\n",
    "This is a toy puzzle: the network will take 4 patch embeddings and output a permutation prediction (0-23).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Prepare all permutations for 2x2 puzzle\n",
    "permutations = list(itertools.permutations(range(4)))\n",
    "\n",
    "# Network for 2x2 jigsaw (shared CNN for patches + permutation classifier)\n",
    "class JigsawNet2x2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.patch_net = nn.Sequential(  # small conv for patch feature\n",
    "            nn.Conv2d(3, 32, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*8*8, 128), nn.ReLU()  # assuming input patch 16x16 (if original 32 and 2x2 grid)\n",
    "        )\n",
    "        self.classifier = nn.Linear(128*4, len(permutations))  # combine 4 patch features\n",
    "    def forward(self, patches):\n",
    "        # patches shape: (batch, 4, 3, 16, 16)\n",
    "        B = patches.size(0)\n",
    "        patches = patches.view(B*4, 3, 16, 16)\n",
    "        feat = self.patch_net(patches)          # shape (B*4, 128)\n",
    "        feat = feat.view(B, 4*128)             # concatenate features\n",
    "        out = self.classifier(feat)           # shape (B, 24) logits for each permutation\n",
    "        return out\n",
    "\n",
    "# Function to create 2x2 jigsaw puzzle from image\n",
    "def make_puzzle_2x2(img):\n",
    "    # img expected size 32x32\n",
    "    # Split into four 16x16 patches\n",
    "    patches = []\n",
    "    # top-left\n",
    "    patches.append(TF.crop(img, 0, 0, 16, 16))\n",
    "    # top-right\n",
    "    patches.append(TF.crop(img, 0, 16, 16, 16))\n",
    "    # bottom-left\n",
    "    patches.append(TF.crop(img, 16, 0, 16, 16))\n",
    "    # bottom-right\n",
    "    patches.append(TF.crop(img, 16, 16, 16, 16))\n",
    "    # Choose a random permutation\n",
    "    perm_idx = torch.randint(len(permutations), (1,)).item()\n",
    "    perm = permutations[perm_idx]\n",
    "    shuffled_patches = [patches[i] for i in perm]\n",
    "    return torch.stack(shuffled_patches), perm_idx\n",
    "\n",
    "# Example usage:\n",
    "img, _ = train_dataset[0]\n",
    "puzzle, perm_label = make_puzzle_2x2(img)\n",
    "print(\"True permutation:\", permutations[perm_label])\n",
    "# Visualize puzzle\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(2,2))\n",
    "for i, patch in enumerate(puzzle):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(patch.permute(1,2,0))\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Shuffled 2x2 Puzzle\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e296439",
   "metadata": {},
   "source": [
    "We could train `JigsawNet2x2` similarly to how we trained `RotationNet`, feeding in puzzles and labels and using cross-entropy. Given the limited scope, we'll not run a full training here. But if we did, the network would learn to predict the correct arrangement out of 24 possibilities.\n",
    "\n",
    "Scaling to 3x3 puzzles: The real jigsaw task uses 9 pieces and a subset of 1000 permutations. That task is quite complex – the network must learn higher-level assembly. By examining intermediate feature activations, one finds the network learns to detect patterns that span multiple patches (e.g., continuation of a fence or a stripe from one patch to the adjacent patch). This encourages a form of contextual understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039be388",
   "metadata": {},
   "source": [
    "## Image Colorization\n",
    "\n",
    "Colorization as a self-supervised task was explored by Larsson et al. 2016 and Zhang et al. 2016. Here, the pretext task is: take a grayscale image and predict the color (the output could be the chromatic channels or full color image). The network is trained on millions of color photos, but during training it only receives the grayscale version as input; the target is the original color image. Essentially, the model learns to add color to black-and-white images.\n",
    "\n",
    "Why is this meaningful? Consider what it takes to colorize an image:\n",
    "- The model needs to infer what objects or materials are present to color them realistically (grass is likely green, sky blue, oranges are orange, etc.).\n",
    "- It also must capture contextual cues: e.g., a football field vs a dry field might have different shades of green/brown.\n",
    "  \n",
    "Some ambiguity is involved (a shirt could be any color), but models address this by predicting a distribution of colors or using classification in color space.\n",
    "\n",
    "Zhang et al. (\"Colorful Image Colorization\") treated colorization as a classification problem in Lab color space:\n",
    "- They use the L (lightness) channel as input, and predict a and b color channels.\n",
    "- Instead of regressing raw color values (which can lead to dull averages), they quantize the ab color space into bins (313 bins) and have the network output a probability distribution over colors for each pixel. A special loss (mix of multinomial cross-entropy with re-balancing) is used to encourage vibrant colors.\n",
    "- At test time, they pick the most likely color for each pixel (or use an annealed mean to avoid desaturation).\n",
    "\n",
    "The colorization network often has an encoder-decoder architecture (to produce an output map the same size as input). By learning to colorize, the encoder part of the network learns rich features about objects – because to color them properly, it must recognize what they are:\n",
    "- If the input is a gray photo of a banana, the network likely outputs yellow tints for that banana region; to do that it must have some representation of \"banana-ness\".\n",
    "- If the input is a gray sky, it knows skies are usually blue or gray; a lawn is green, etc.\n",
    "\n",
    "Colorization was shown to be a strong supervisory signal. Zhang et al. reported that their colorization-pretrained model, when used as a feature extractor, performed well on classification and detection tasks (not as well as supervised pre-training, but impressively close for an unsupervised method of that time).\n",
    "\n",
    "**Visualization:** Colorization networks produce impressive visual results – turning old black & white photos to color. But keep in mind, for self-supervision, we don't actually need perfect photo-realistic colorization; we only care that the network learned good internal representations. Sometimes the output might be nonsensical (because multiple color assignments are plausible), but as long as the process forced the network to learn structure (e.g., outline of objects, texture details), it succeeded as a pretext.\n",
    "\n",
    "Let's do a quick code sketch using a pre-trained colorization model from OpenCV to see how colorization works (OpenCV has a deep learning colorizer example trained by Richard Zhang): (We assume we have an OpenCV dnn model or similar. If not, we'll skip actual code execution and just conceptually outline.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09781e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Suppose we have a pre-trained colorization model loaded (this is pseudo-code)\n",
    "net = cv2.dnn.readNetFromCaffe(\"colorization_deploy_v2.prototxt\", \"colorization_release_v2.caffemodel\")\n",
    "# The network expects input in Lab space with L normalized a certain way.\n",
    "# We prepare a grayscale image:\n",
    "gray = cv2.imread('grayscale_image.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "gray = cv2.cvtColor(gray, cv2.COLOR_GRAY2RGB)  # replicate to 3 channels for input\n",
    "blob = cv2.dnn.blobFromImage(gray, scalefactor=1.0/255.0)\n",
    "net.setInput(blob)\n",
    "output = net.forward()  # output ab channels\n",
    "# Post-process the output with the original L channel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e93e9",
   "metadata": {},
   "source": [
    "## Summary and Usage\n",
    "\n",
    "Each of these tasks – rotation, jigsaw, colorization – can be used to pre-train a model on unlabeled images. Typically, one would:\n",
    "- Set up the chosen pretext task and train the network until it solves that task well (convergence of loss).\n",
    "- Then either directly use the learned features or fine-tune the network on the actual task of interest.\n",
    "\n",
    "In practice, these tasks can also be combined. For example, one could train a network to simultaneously predict rotation and colorize an image (multi-task self-supervision) hoping to learn an even stronger representation. Some works added rotation prediction heads on contrastive learning models (like an auxiliary loss) to boost performance. While these pretext tasks have somewhat been overtaken by contrastive and transformer-based methods in recent years (which achieve higher results), they are still very relevant:\n",
    "- They are simple and lightweight to implement (no need for large batches or tricky losses).\n",
    "- They provide intuition about what features are learned (e.g., one can inspect a rotation model and see neurons firing for upright vs inverted patterns).\n",
    "- They might be useful in scenarios where contrastive learning is hard to apply (e.g., small datasets), or can serve as a quick initialization.\n",
    "\n",
    "In the next notebook, we'll shift focus to masked modeling, which is a pretext task paradigm that has revolutionized NLP (with BERT) and is now making waves in vision (with Masked Autoencoders). This will connect the ideas from this notebook (predicting missing information) with powerful modern architectures.\n",
    "\n",
    "**Bonus Exercise:** Design your own jigsaw puzzle variant: for instance, a 5x5 puzzle but have the network predict the relative location of each patch (a series of pairwise position classifications). How might that be set up? Alternatively, think about video jigsaw – shuffling frames in a video and predicting the correct order. What would a model have to learn in order to solve a video jigsaw puzzle?\n",
    "\n",
    "## References:\n",
    "- Gidaris, S., Singh, P., Komodakis, N. (2018). \"Unsupervised Representation Learning by Predicting Image Rotations.\" ICLR. – Rotation prediction task introduced.\n",
    "- Noroozi, M., Favaro, P. (2016). \"Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles.\" ECCV. – Jigsaw puzzle task for SSL.\n",
    "- Zhang, R., Isola, P., Efros, A.A. (2016). \"Colorful Image Colorization.\" ECCV. – Treats colorization as self-supervision, with a class-balanced loss for vibrant outputs.\n",
    "- Larsson, G. et al. (2016). \"Learning Representations for Automatic Colorization.\" ECCV. – Another approach to colorization-based SSL.\n",
    "- Doersch, C., Gupta, A., Efros, A.A. (2015). \"Unsupervised Visual Representation Learning by Context Prediction.\" ICCV. – Early work predicting relative patch positions (the precursor to jigsaw idea).\n",
    "- Pathak, D. et al. (2016). \"Context Encoders: Feature Learning by Inpainting.\" CVPR. – Another pretext task: image inpainting (predict missing central patch), which is akin to colorization in spirit.\n",
    "- Reed, J. et al. (2019). \"Self-supervised Learning for Video: Sequence Ordering.\" – Extends the puzzle idea to video frames (temporal order).\n",
    "- Jenni, S., Favaro, P. (2018). \"Self-Supervised Feature Learning by Learning to Spot Artifacts.\" – A creative task where artifacts (like inpainting with noise) are introduced and the network must detect them; another form of self-supervision.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
